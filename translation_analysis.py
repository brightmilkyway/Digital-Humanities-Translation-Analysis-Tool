#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¿»è¯‘æ–‡æœ¬æ•°å­—äººæ–‡åˆ†æå·¥å…·
Digital Humanities Translation Analysis Tool

åŠŸèƒ½ï¼š
1. æ–‡æœ¬ç»Ÿè®¡åˆ†æï¼ˆè¯é¢‘ã€å¥é•¿ã€æ®µè½ç»“æ„ç­‰ï¼‰
2. ç¿»è¯‘ç­–ç•¥åˆ†æï¼ˆç›´è¯‘vsæ„è¯‘å€¾å‘ï¼‰
3. æ–‡ä½“é£æ ¼å¯¹æ¯”
4. è¯­è¨€å¤æ‚åº¦åˆ†æ
5. å¯è§†åŒ–å±•ç¤ºå’ŒæŠ¥å‘Šç”Ÿæˆ

Author:è¢ç¦¹è±ª&Claude Sonnet 4
"""

import re
import jieba
import jieba.analyse
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from collections import Counter, defaultdict
from textstat import flesch_reading_ease, flesch_kincaid_grade
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from wordcloud import WordCloud
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

class TranslationAnalyzer:
    def __init__(self, source_file, target_file, source_lang='en', target_lang='zh'):
        """
        åˆå§‹åŒ–ç¿»è¯‘åˆ†æå™¨
        
        Args:
            source_file: åŸæ–‡æ–‡ä»¶è·¯å¾„
            target_file: è¯‘æ–‡æ–‡ä»¶è·¯å¾„
            source_lang: æºè¯­è¨€ä»£ç 
            target_lang: ç›®æ ‡è¯­è¨€ä»£ç 
        """
        self.source_lang = source_lang
        self.target_lang = target_lang
        
        # è¯»å–æ–‡æœ¬
        with open(source_file, 'r', encoding='utf-8') as f:
            self.source_text = f.read()
        with open(target_file, 'r', encoding='utf-8') as f:
            self.target_text = f.read()
        
        # åˆå§‹åŒ–åˆ†æç»“æœå­˜å‚¨
        self.analysis_results = {}
        
        # ä¸‹è½½å¿…è¦çš„NLTKæ•°æ®
        try:
            nltk.data.find('tokenizers/punkt')
        except LookupError:
            nltk.download('punkt')
        try:
            nltk.data.find('corpora/stopwords')
        except LookupError:
            nltk.download('stopwords')
    
    def basic_statistics(self):
        """åŸºç¡€æ–‡æœ¬ç»Ÿè®¡"""
        print("æ­£åœ¨è¿›è¡ŒåŸºç¡€ç»Ÿè®¡åˆ†æ...")
        
        # è‹±æ–‡ç»Ÿè®¡
        source_sentences = sent_tokenize(self.source_text)
        source_words = word_tokenize(self.source_text.lower())
        source_words = [w for w in source_words if w.isalpha()]
        
        # ä¸­æ–‡ç»Ÿè®¡
        target_sentences = re.split(r'[ã€‚ï¼ï¼Ÿï¼›]', self.target_text)
        target_sentences = [s.strip() for s in target_sentences if s.strip()]
        target_words = list(jieba.cut(self.target_text))
        target_words = [w.strip() for w in target_words if w.strip() and len(w) > 1]
        
        stats = {
            'source': {
                'characters': len(self.source_text),
                'words': len(source_words),
                'sentences': len(source_sentences),
                'paragraphs': len([p for p in self.source_text.split('\n\n') if p.strip()]),
                'avg_sentence_length': np.mean([len(word_tokenize(s)) for s in source_sentences]),
                'avg_word_length': np.mean([len(w) for w in source_words])
            },
            'target': {
                'characters': len(self.target_text),
                'words': len(target_words),
                'sentences': len(target_sentences),
                'paragraphs': len([p for p in self.target_text.split('\n\n') if p.strip()]),
                'avg_sentence_length': np.mean([len(list(jieba.cut(s))) for s in target_sentences if s]),
                'avg_word_length': np.mean([len(w) for w in target_words])
            }
        }
        
        # è®¡ç®—ç¿»è¯‘æ¯”ç‡
        stats['ratios'] = {
            'character_ratio': stats['target']['characters'] / stats['source']['characters'],
            'word_ratio': stats['target']['words'] / stats['source']['words'],
            'sentence_ratio': stats['target']['sentences'] / stats['source']['sentences']
        }
        
        self.analysis_results['basic_stats'] = stats
        return stats
    
    def lexical_analysis(self):
        """è¯æ±‡åˆ†æ"""
        print("æ­£åœ¨è¿›è¡Œè¯æ±‡åˆ†æ...")
        
        # è‹±æ–‡è¯æ±‡åˆ†æ
        source_words = word_tokenize(self.source_text.lower())
        source_words = [w for w in source_words if w.isalpha()]
        stop_words = set(stopwords.words('english'))
        source_content_words = [w for w in source_words if w not in stop_words]
        
        # ä¸­æ–‡è¯æ±‡åˆ†æ
        target_words = list(jieba.cut(self.target_text))
        target_words = [w.strip() for w in target_words if w.strip() and len(w) > 1]
        
        # è¯é¢‘ç»Ÿè®¡
        source_freq = Counter(source_content_words)
        target_freq = Counter(target_words)
        
        # è¯æ±‡ä¸°å¯Œåº¦ï¼ˆTTR - Type Token Ratioï¼‰
        source_ttr = len(set(source_content_words)) / len(source_content_words) if source_content_words else 0
        target_ttr = len(set(target_words)) / len(target_words) if target_words else 0
        
        lexical_data = {
            'source_top_words': source_freq.most_common(20),
            'target_top_words': target_freq.most_common(20),
            'source_ttr': source_ttr,
            'target_ttr': target_ttr,
            'source_unique_words': len(set(source_content_words)),
            'target_unique_words': len(set(target_words))
        }
        
        self.analysis_results['lexical'] = lexical_data
        return lexical_data
    
    def complexity_analysis(self):
        """å¤æ‚åº¦åˆ†æ"""
        print("æ­£åœ¨è¿›è¡Œå¤æ‚åº¦åˆ†æ...")
        
        # è‹±æ–‡å¯è¯»æ€§æŒ‡æ ‡
        source_flesch = flesch_reading_ease(self.source_text)
        source_fk_grade = flesch_kincaid_grade(self.source_text)
        
        # å¥å­é•¿åº¦åˆ†å¸ƒ
        source_sentences = sent_tokenize(self.source_text)
        target_sentences = re.split(r'[ã€‚ï¼ï¼Ÿï¼›]', self.target_text)
        target_sentences = [s.strip() for s in target_sentences if s.strip()]
        
        source_sent_lengths = [len(word_tokenize(s)) for s in source_sentences]
        target_sent_lengths = [len(list(jieba.cut(s))) for s in target_sentences if s]
        
        complexity_data = {
            'source_flesch_score': source_flesch,
            'source_fk_grade': source_fk_grade,
            'source_sent_lengths': source_sent_lengths,
            'target_sent_lengths': target_sent_lengths,
            'source_avg_sent_length': np.mean(source_sent_lengths),
            'target_avg_sent_length': np.mean(target_sent_lengths),
            'source_sent_length_std': np.std(source_sent_lengths),
            'target_sent_length_std': np.std(target_sent_lengths)
        }
        
        self.analysis_results['complexity'] = complexity_data
        return complexity_data
    
    def translation_strategy_analysis(self):
        """ç¿»è¯‘ç­–ç•¥åˆ†æ"""
        print("æ­£åœ¨è¿›è¡Œç¿»è¯‘ç­–ç•¥åˆ†æ...")
        
        # åŸºäºå¥å­å¯¹é½è¿›è¡Œåˆ†æ
        source_sentences = sent_tokenize(self.source_text)
        target_sentences = re.split(r'[ã€‚ï¼ï¼Ÿï¼›]', self.target_text)
        target_sentences = [s.strip() for s in target_sentences if s.strip()]
        
        # ç®€å•çš„å¥å­é•¿åº¦æ¯”è¾ƒæ¥æ¨æ–­ç¿»è¯‘ç­–ç•¥
        length_ratios = []
        min_len = min(len(source_sentences), len(target_sentences))
        
        for i in range(min_len):
            source_len = len(word_tokenize(source_sentences[i]))
            target_len = len(list(jieba.cut(target_sentences[i])))
            if source_len > 0:
                ratio = target_len / source_len
                length_ratios.append(ratio)
        
        # ç¿»è¯‘ç­–ç•¥æŒ‡æ ‡
        avg_ratio = np.mean(length_ratios) if length_ratios else 1
        ratio_variance = np.var(length_ratios) if length_ratios else 0
        
        # æ ¹æ®æ¯”ç‡æ¨æ–­ç¿»è¯‘å€¾å‘
        if avg_ratio > 1.2:
            strategy_tendency = "æ„è¯‘å€¾å‘ï¼ˆæ‰©å±•æ€§ç¿»è¯‘ï¼‰"
        elif avg_ratio < 0.8:
            strategy_tendency = "ç›´è¯‘å€¾å‘ï¼ˆå‹ç¼©æ€§ç¿»è¯‘ï¼‰"
        else:
            strategy_tendency = "å¹³è¡¡ç¿»è¯‘ç­–ç•¥"
        
        strategy_data = {
            'length_ratios': length_ratios,
            'avg_length_ratio': avg_ratio,
            'ratio_variance': ratio_variance,
            'strategy_tendency': strategy_tendency,
            'sentence_alignment_quality': min_len / max(len(source_sentences), len(target_sentences))
        }
        
        self.analysis_results['strategy'] = strategy_data
        return strategy_data
    
    def create_visualizations(self):
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
        print("æ­£åœ¨ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        
        # åˆ›å»ºå›¾è¡¨ç›®å½•
        import os
        if not os.path.exists('translation_analysis_charts'):
            os.makedirs('translation_analysis_charts')
        
        # 1. åŸºç¡€ç»Ÿè®¡å¯¹æ¯”å›¾
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # æ–‡æœ¬é‡å¯¹æ¯”
        stats = self.analysis_results['basic_stats']
        categories = ['å­—ç¬¦æ•°', 'è¯æ•°', 'å¥æ•°', 'æ®µè½æ•°']
        source_values = [stats['source']['characters'], stats['source']['words'], 
                        stats['source']['sentences'], stats['source']['paragraphs']]
        target_values = [stats['target']['characters'], stats['target']['words'], 
                        stats['target']['sentences'], stats['target']['paragraphs']]
        
        x = np.arange(len(categories))
        width = 0.35
        
        axes[0,0].bar(x - width/2, source_values, width, label='åŸæ–‡', alpha=0.8)
        axes[0,0].bar(x + width/2, target_values, width, label='è¯‘æ–‡', alpha=0.8)
        axes[0,0].set_title('æ–‡æœ¬é‡ç»Ÿè®¡å¯¹æ¯”')
        axes[0,0].set_xticks(x)
        axes[0,0].set_xticklabels(categories)
        axes[0,0].legend()
        
        # 2. å¥é•¿åˆ†å¸ƒå¯¹æ¯”
        complexity = self.analysis_results['complexity']
        axes[0,1].hist(complexity['source_sent_lengths'], bins=20, alpha=0.7, label='åŸæ–‡å¥é•¿')
        axes[0,1].hist(complexity['target_sent_lengths'], bins=20, alpha=0.7, label='è¯‘æ–‡å¥é•¿')
        axes[0,1].set_title('å¥å­é•¿åº¦åˆ†å¸ƒ')
        axes[0,1].set_xlabel('å¥å­é•¿åº¦ï¼ˆè¯æ•°ï¼‰')
        axes[0,1].set_ylabel('é¢‘æ¬¡')
        axes[0,1].legend()
        
        # 3. TTRå¯¹æ¯”
        lexical = self.analysis_results['lexical']
        ttr_data = ['åŸæ–‡TTR', 'è¯‘æ–‡TTR']
        ttr_values = [lexical['source_ttr'], lexical['target_ttr']]
        axes[1,0].bar(ttr_data, ttr_values, color=['skyblue', 'lightcoral'])
        axes[1,0].set_title('è¯æ±‡ä¸°å¯Œåº¦å¯¹æ¯”ï¼ˆTTRï¼‰')
        axes[1,0].set_ylabel('TTRå€¼')
        
        # 4. ç¿»è¯‘ç­–ç•¥åˆ†æ
        if 'strategy' in self.analysis_results:
            strategy = self.analysis_results['strategy']
            if strategy['length_ratios']:
                axes[1,1].hist(strategy['length_ratios'], bins=15, alpha=0.7, color='green')
                axes[1,1].axvline(strategy['avg_length_ratio'], color='red', linestyle='--', 
                                label=f'å¹³å‡æ¯”ç‡: {strategy["avg_length_ratio"]:.2f}')
                axes[1,1].set_title('å¥é•¿æ¯”ç‡åˆ†å¸ƒï¼ˆè¯‘æ–‡/åŸæ–‡ï¼‰')
                axes[1,1].set_xlabel('é•¿åº¦æ¯”ç‡')
                axes[1,1].set_ylabel('é¢‘æ¬¡')
                axes[1,1].legend()
        
        plt.tight_layout()
        plt.savefig('translation_analysis_charts/comprehensive_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        # 5. è¯äº‘å›¾
        self._create_wordclouds()
        
        # 6. äº¤äº’å¼å›¾è¡¨
        self._create_interactive_charts()
    
    def _create_wordclouds(self):
        """åˆ›å»ºè¯äº‘å›¾"""
        fig, axes = plt.subplots(1, 2, figsize=(20, 8))
        
        # åŸæ–‡è¯äº‘
        source_words = word_tokenize(self.source_text.lower())
        source_words = [w for w in source_words if w.isalpha() and len(w) > 3]
        stop_words = set(stopwords.words('english'))
        source_words = [w for w in source_words if w not in stop_words]
        source_text_clean = ' '.join(source_words)
        
        if source_text_clean:
            wordcloud_en = WordCloud(width=800, height=400, background_color='white').generate(source_text_clean)
            axes[0].imshow(wordcloud_en, interpolation='bilinear')
            axes[0].set_title('åŸæ–‡è¯äº‘å›¾', fontsize=16)
            axes[0].axis('off')
        
        # è¯‘æ–‡è¯äº‘
        target_words = jieba.cut(self.target_text)
        target_words = [w for w in target_words if len(w) > 1 and w.strip()]
        target_text_clean = ' '.join(target_words)
        
        if target_text_clean:
            wordcloud_zh = WordCloud(font_path='simhei.ttf', width=800, height=400, 
                                   background_color='white').generate(target_text_clean)
            axes[1].imshow(wordcloud_zh, interpolation='bilinear')
            axes[1].set_title('è¯‘æ–‡è¯äº‘å›¾', fontsize=16)
            axes[1].axis('off')
        
        plt.tight_layout()
        plt.savefig('translation_analysis_charts/wordclouds.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def _create_interactive_charts(self):
        """åˆ›å»ºäº¤äº’å¼å›¾è¡¨"""
        # è¯é¢‘å¯¹æ¯”å›¾
        lexical = self.analysis_results['lexical']
        
        source_words, source_freqs = zip(*lexical['source_top_words'][:15])
        target_words, target_freqs = zip(*lexical['target_top_words'][:15])
        
        fig = make_subplots(rows=1, cols=2, subplot_titles=('åŸæ–‡é«˜é¢‘è¯', 'è¯‘æ–‡é«˜é¢‘è¯'))
        
        fig.add_trace(go.Bar(x=list(source_words), y=list(source_freqs), name='åŸæ–‡'), row=1, col=1)
        fig.add_trace(go.Bar(x=list(target_words), y=list(target_freqs), name='è¯‘æ–‡'), row=1, col=2)
        
        fig.update_layout(title_text="é«˜é¢‘è¯å¯¹æ¯”", showlegend=False)
        fig.write_html('translation_analysis_charts/word_frequency_comparison.html')
    
    def generate_report(self):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        print("æ­£åœ¨ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
        
        report = f"""è¯‘æ–‡æœ¬æ•°å­—äººæ–‡åˆ†ææŠ¥å‘Š
Translation Digital Humanities Analysis Report

1. åŸºç¡€ç»Ÿè®¡åˆ†æ

åŸæ–‡ç»Ÿè®¡ï¼š
- å­—ç¬¦æ•°ï¼š{self.analysis_results['basic_stats']['source']['characters']:,}
- è¯æ•°ï¼š{self.analysis_results['basic_stats']['source']['words']:,}
- å¥æ•°ï¼š{self.analysis_results['basic_stats']['source']['sentences']:,}
- æ®µè½æ•°ï¼š{self.analysis_results['basic_stats']['source']['paragraphs']:,}
- å¹³å‡å¥é•¿ï¼š{self.analysis_results['basic_stats']['source']['avg_sentence_length']:.2f} è¯/å¥
- å¹³å‡è¯é•¿ï¼š{self.analysis_results['basic_stats']['source']['avg_word_length']:.2f} å­—ç¬¦/è¯

è¯‘æ–‡ç»Ÿè®¡ï¼š
- å­—ç¬¦æ•°ï¼š{self.analysis_results['basic_stats']['target']['characters']:,}
- è¯æ•°ï¼š{self.analysis_results['basic_stats']['target']['words']:,}
- å¥æ•°ï¼š{self.analysis_results['basic_stats']['target']['sentences']:,}
- æ®µè½æ•°ï¼š{self.analysis_results['basic_stats']['target']['paragraphs']:,}
- å¹³å‡å¥é•¿ï¼š{self.analysis_results['basic_stats']['target']['avg_sentence_length']:.2f} è¯/å¥
- å¹³å‡è¯é•¿ï¼š{self.analysis_results['basic_stats']['target']['avg_word_length']:.2f} å­—ç¬¦/è¯

ç¿»è¯‘æ¯”ç‡ï¼š
- å­—ç¬¦æ¯”ç‡ï¼š{self.analysis_results['basic_stats']['ratios']['character_ratio']:.2f}
- è¯æ±‡æ¯”ç‡ï¼š{self.analysis_results['basic_stats']['ratios']['word_ratio']:.2f}
- å¥å­æ¯”ç‡ï¼š{self.analysis_results['basic_stats']['ratios']['sentence_ratio']:.2f}

2. è¯æ±‡åˆ†æ

è¯æ±‡ä¸°å¯Œåº¦ï¼ˆTTRï¼‰ï¼š
- åŸæ–‡TTRï¼š{self.analysis_results['lexical']['source_ttr']:.4f}
- è¯‘æ–‡TTRï¼š{self.analysis_results['lexical']['target_ttr']:.4f}

ç‹¬ç‰¹è¯æ±‡æ•°ï¼š
- åŸæ–‡ï¼š{self.analysis_results['lexical']['source_unique_words']:,}
- è¯‘æ–‡ï¼š{self.analysis_results['lexical']['target_unique_words']:,}

3. å¤æ‚åº¦åˆ†æ

åŸæ–‡å¯è¯»æ€§ï¼š
- Fleschå¯è¯»æ€§å¾—åˆ†ï¼š{self.analysis_results['complexity']['source_flesch_score']:.2f}
- Flesch-Kincaidç­‰çº§ï¼š{self.analysis_results['complexity']['source_fk_grade']:.2f}

å¥å­å¤æ‚åº¦ï¼š
- åŸæ–‡å¹³å‡å¥é•¿ï¼š{self.analysis_results['complexity']['source_avg_sent_length']:.2f} Â± {self.analysis_results['complexity']['source_sent_length_std']:.2f}
- è¯‘æ–‡å¹³å‡å¥é•¿ï¼š{self.analysis_results['complexity']['target_avg_sent_length']:.2f} Â± {self.analysis_results['complexity']['target_sent_length_std']:.2f}

4. ç¿»è¯‘ç­–ç•¥åˆ†æ

"""
        
        if 'strategy' in self.analysis_results:
            strategy = self.analysis_results['strategy']
            report += f"""ç¿»è¯‘ç­–ç•¥å€¾å‘ï¼š {strategy['strategy_tendency']}

å…³é”®æŒ‡æ ‡ï¼š
- å¹³å‡é•¿åº¦æ¯”ç‡ï¼š{strategy['avg_length_ratio']:.2f}
- æ¯”ç‡æ–¹å·®ï¼š{strategy['ratio_variance']:.4f}
- å¥å­å¯¹é½è´¨é‡ï¼š{strategy['sentence_alignment_quality']:.2f}

5. åˆ†æç»“è®º

åŸºäºä»¥ä¸Šæ•°æ®åˆ†æï¼Œæ‚¨çš„ç¿»è¯‘å‘ˆç°ä»¥ä¸‹ç‰¹ç‚¹ï¼š

1. æ–‡æœ¬æ‰©å±•æ€§ï¼šè¯‘æ–‡ç›¸å¯¹åŸæ–‡çš„å­—ç¬¦æ¯”ç‡ä¸º {self.analysis_results['basic_stats']['ratios']['character_ratio']:.2f}ï¼Œè¡¨æ˜ç¿»è¯‘è¿‡ç¨‹ä¸­æ–‡æœ¬é•¿åº¦{'å¢åŠ ' if self.analysis_results['basic_stats']['ratios']['character_ratio'] > 1 else 'å‡å°‘'}äº† {abs(self.analysis_results['basic_stats']['ratios']['character_ratio'] - 1) * 100:.1f}%

2. è¯æ±‡ä¸°å¯Œåº¦ï¼šè¯‘æ–‡TTRä¸º {self.analysis_results['lexical']['target_ttr']:.4f}ï¼Œ{'é«˜äº' if self.analysis_results['lexical']['target_ttr'] > self.analysis_results['lexical']['source_ttr'] else 'ä½äº'}åŸæ–‡çš„ {self.analysis_results['lexical']['source_ttr']:.4f}

3.*ç¿»è¯‘ç­–ç•¥ï¼š{strategy['strategy_tendency']}ï¼Œå¹³å‡å¥é•¿æ¯”ç‡ä¸º {strategy['avg_length_ratio']:.2f}

4. å¯è¯»æ€§ï¼šåŸæ–‡Fleschå¾—åˆ†ä¸º {self.analysis_results['complexity']['source_flesch_score']:.1f}ï¼Œå±äº{'å®¹æ˜“' if self.analysis_results['complexity']['source_flesch_score'] > 70 else 'ä¸­ç­‰' if self.analysis_results['complexity']['source_flesch_score'] > 30 else 'å›°éš¾'}é˜…è¯»æ°´å¹³

6. ç¿»è¯‘è´¨é‡è¯„ä¼°å»ºè®®

- è€ƒè™‘å¥å­é•¿åº¦çš„å‡è¡¡æ€§ï¼Œé¿å…è¿‡é•¿æˆ–è¿‡çŸ­çš„å¥å­
- æ³¨æ„ä¿æŒé€‚å½“çš„è¯æ±‡ä¸°å¯Œåº¦
- æ ¹æ®æ–‡æœ¬ç±»å‹è°ƒæ•´ç¿»è¯‘ç­–ç•¥ï¼ˆç›´è¯‘vsæ„è¯‘ï¼‰
- å…³æ³¨è¯‘æ–‡çš„æµç•…æ€§å’Œå¯è¯»æ€§

---
æœ¬æŠ¥å‘Šç”±æ•°å­—äººæ–‡ç¿»è¯‘åˆ†æå·¥å…·ç”Ÿæˆ
"""
        
        # ä¿å­˜æŠ¥å‘Š
        with open('translation_analysis_report.md', 'w', encoding='utf-8') as f:
            f.write(report)
        
        print("åˆ†ææŠ¥å‘Šå·²ä¿å­˜ä¸º 'translation_analysis_report.md'")
        return report
    
    def run_full_analysis(self):
        """è¿è¡Œå®Œæ•´åˆ†ææµç¨‹"""
        print("å¼€å§‹ç¿»è¯‘æ–‡æœ¬æ•°å­—äººæ–‡åˆ†æ...")
        print("=" * 50)
        
        # æ‰§è¡Œå„é¡¹åˆ†æ
        self.basic_statistics()
        self.lexical_analysis()
        self.complexity_analysis()
        self.translation_strategy_analysis()
        
        # ç”Ÿæˆå¯è§†åŒ–
        self.create_visualizations()
        
        # ç”ŸæˆæŠ¥å‘Š
        report = self.generate_report()
        
        print("=" * 50)
        print("åˆ†æå®Œæˆï¼ç”Ÿæˆçš„æ–‡ä»¶ï¼š")
        print("- translation_analysis_report.md (åˆ†ææŠ¥å‘Š)")
        print("- translation_analysis_charts/ (å¯è§†åŒ–å›¾è¡¨)")
        print("  â”œâ”€â”€ comprehensive_analysis.png")
        print("  â”œâ”€â”€ wordclouds.png")
        print("  â””â”€â”€ word_frequency_comparison.html")
        
        return self.analysis_results


def main():
    """ä¸»å‡½æ•° - ä½¿ç”¨ç¤ºä¾‹"""
    print("=" * 50)
    print("ğŸ” ç¿»è¯‘æ–‡æœ¬æ•°å­—äººæ–‡åˆ†æå·¥å…·")
    print("   Translation Digital Humanities Analysis Tool")
    print("=" * 50)
    
    # ========== åœ¨è¿™é‡Œç›´æ¥æŒ‡å®šæ–‡ä»¶è·¯å¾„ ==========
    source_file = r"E:\å—å¼€\è‹±è¯­ä¸“ä¸š\æ–‡å­¦ç¿»è¯‘\æœŸæœ«é¡¹ç›®\source.txt"     # åŸæ–‡æ–‡ä»¶è·¯å¾„
    target_file = r"E:\å—å¼€\è‹±è¯­ä¸“ä¸š\æ–‡å­¦ç¿»è¯‘\æœŸæœ«é¡¹ç›®\target.txt"     # è¯‘æ–‡æ–‡ä»¶è·¯å¾„
    # ==========================================
    
    print(f"ğŸ“ åˆ†ææ–‡ä»¶ï¼š")
    print(f"   åŸæ–‡æ–‡ä»¶: {source_file}")
    print(f"   è¯‘æ–‡æ–‡ä»¶: {target_file}")
    
    try:
        print("\nğŸš€ å¼€å§‹åˆ†æ...")
        
        # åˆ›å»ºåˆ†æå™¨å®ä¾‹
        analyzer = TranslationAnalyzer(source_file, target_file)
        
        # è¿è¡Œå®Œæ•´åˆ†æ
        results = analyzer.run_full_analysis()
        
        print("\n" + "=" * 50)
        print("ğŸ“Š åˆ†æç»“æœé¢„è§ˆï¼š")
        print(f"   åŸæ–‡è¯æ•°: {results['basic_stats']['source']['words']:,}")
        print(f"   è¯‘æ–‡è¯æ•°: {results['basic_stats']['target']['words']:,}")
        print(f"   è¯æ±‡æ¯”ç‡: {results['basic_stats']['ratios']['word_ratio']:.2f}")
        print(f"   ç¿»è¯‘ç­–ç•¥: {results['strategy']['strategy_tendency']}")
        print("=" * 50)
        print("âœ¨ åˆ†æå®Œæˆï¼è¯·æŸ¥çœ‹ç”Ÿæˆçš„æŠ¥å‘Šå’Œå›¾è¡¨æ–‡ä»¶ã€‚")
        
    except FileNotFoundError as e:
        print(f"\nâŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶")
        print(f"   è¯·ç¡®ä¿ä»¥ä¸‹æ–‡ä»¶å­˜åœ¨ï¼š")
        print(f"   - {source_file}")
        print(f"   - {target_file}")
    except Exception as e:
        print(f"\nâŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š{str(e)}")
        print("è¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®ï¼ˆåº”ä¸ºUTF-8ç¼–ç çš„æ–‡æœ¬æ–‡ä»¶ï¼‰")


if __name__ == "__main__":
    main()
